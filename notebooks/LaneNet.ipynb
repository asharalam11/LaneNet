{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LaneNet.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNYb55aT6BzhzijP0LPfbM5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"7d1f17bd25354d64b8875765941392f8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2a636c0309d74e18a6ffa0aff6a570ac","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7beff0e6b7c64c228702bd889aba5397","IPY_MODEL_45d6aa555d434495a2a30c7f869645f8"]}},"2a636c0309d74e18a6ffa0aff6a570ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7beff0e6b7c64c228702bd889aba5397":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f54e8a0c62574bdc8bccc5d52080a68e","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":553507836,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":553507836,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0686325891204464ba65e5f984643c93"}},"45d6aa555d434495a2a30c7f869645f8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_86151d47bd95482d803ad2a9b313fc67","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 528M/528M [00:22&lt;00:00, 24.9MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ffc2a8d30278410b8e378d5e70029f07"}},"f54e8a0c62574bdc8bccc5d52080a68e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0686325891204464ba65e5f984643c93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"86151d47bd95482d803ad2a9b313fc67":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ffc2a8d30278410b8e378d5e70029f07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"QeV3ULVuvOwr","colab_type":"text"},"source":["#### * _We used Pytorch for our CNN models_ *"]},{"cell_type":"markdown","metadata":{"id":"vmerq4CX4wXB","colab_type":"text"},"source":["# __Why Detect Lane Markings__\n","For our project, we are interested in working on lane detection for autonomous vehicles using lane markings painted on the roads. The task of lane detection is still vital to the success of autonomous vehicles because it is pertinent that an autonomous vehicle can exactly recognize which lane it is in and where it needs to go. If it is not able to do that, the vehicles might be going in the middle of two lanes which can be dangerous for other vehicles in the vicinity. Also, lane detection is inevitable to perform maneuvers like lane changes and also for predicting other vehicles’ behavior."]},{"cell_type":"markdown","metadata":{"id":"fKhKOATu5O3h","colab_type":"text"},"source":["# __Build a lane extractor from an image__\n","In this project, we tried to train a Convolutional Neural Network (CNN) to extract lane markings as features from given images of roads with lanes. \n","\n","### __Dataset Description__\n","We plan on using the __CULane Dataset__ (Chinese University Lane Dataset) which contains 88,880 images for the training set, 9,675 images for the validation set and 34,680 images for the test set. Furthermore, the test set is divided into one normal and 8 challenging categories that tests the network on harder images. The dataset is desirable because it contains images from a variety of settings like crowded, night, no line, shadow, curve, etc. Such diversity in the dataset is desirable for training the model.\t\n","\n","### __Modelling Pipeline__\n","  1. Extract the images from the dataset stored on Drive\n","  2. Organize the images into raw images and labelled value\n","  3. Train the Model\n","  4. Get a reasonable validation accuracy\n","  5. Test the Model on unseen images "]},{"cell_type":"code","metadata":{"id":"dl2uVXeSSKs5","colab_type":"code","outputId":"482c98e9-0243-482f-f322-8a12d34d33af","executionInfo":{"status":"ok","timestamp":1589889594404,"user_tz":420,"elapsed":26373,"user":{"displayName":"ASHAR ALAM","photoUrl":"","userId":"03610029456235520310"}},"colab":{"base_uri":"https://localhost:8080/","height":126}},"source":["# this mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","from os.path import join\n","import os\n","\n","ROOT = '/content/drive'     # default for the drive\n","CODE = 'My Drive/cs231n/Project/LaneNet'       # path to our code on Drive\n","DATASET = 'My Drive/cs231n/Project/dataset/trainset'\n","EXPERIMENT = 'My Drive/cs231n/Project/LaneNet/config'\n","MODEL_SAVE = 'My Drive/cs231n/Project'\n","PRETRAINED = 'My Drive/cs231n/Project/pretrained'\n","\n","CODE_PATH = join(ROOT, CODE)\n","DATASET_PATH = join(ROOT, DATASET)\n","EXPERIMENT_CONFIG = join(ROOT, EXPERIMENT)\n","MODEL_SAVE = join(ROOT, MODEL_SAVE)\n","PRETRAINED_PATH = join(ROOT, PRETRAINED)\n","\n","# now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('{}'.format(CODE_PATH))\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"e5MbCxwYosSK","colab_type":"text"},"source":["## Some Imports\n"]},{"cell_type":"code","metadata":{"id":"bpBO4d7C1zsZ","colab_type":"code","colab":{}},"source":["import json\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fra934wNp3jN","colab_type":"text"},"source":["### Let's visualize some of the images we have\n","We have a contiguous set of 20 images frame-by-frame for each scene, with the 20th frame labelled in our dataset. <br>\n","Let's visualize these images to understand what each scene information contains:\n"]},{"cell_type":"code","metadata":{"id":"4ZCiD7TQp7bP","colab_type":"code","outputId":"bed1be33-c619-4e2a-d755-735e81b3f28a","executionInfo":{"status":"ok","timestamp":1589849758610,"user_tz":420,"elapsed":1575,"user":{"displayName":"ASHAR ALAM","photoUrl":"","userId":"03610029456235520310"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["# Obtaining a list of paths for each image\n","\"\"\"\n","FIXED_PATH = DATASET_PATH + '/clips/0313-1/60/'\n","items = os.listdir(FIXED_PATH)\n","n_images = len(items)\n","plt_idx = 0\n","fig = plt.figure()\n","n_cols = 20\n","for image in items: \n","  plt_idx += 1\n","  full_path = FIXED_PATH + image\n","  a = fig.add_subplot(n_cols, np.ceil(n_images/float(n_cols)), plt_idx)\n","  img = plt.imread(full_path)\n","  #plt.imshow(img.astype('uint8'))\n","fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n","plt.show()\n","\"\"\""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nFIXED_PATH = DATASET_PATH + '/clips/0313-1/60/'\\nitems = os.listdir(FIXED_PATH)\\nn_images = len(items)\\nplt_idx = 0\\nfig = plt.figure()\\nn_cols = 20\\nfor image in items: \\n  plt_idx += 1\\n  full_path = FIXED_PATH + image\\n  a = fig.add_subplot(n_cols, np.ceil(n_images/float(n_cols)), plt_idx)\\n  img = plt.imread(full_path)\\n  #plt.imshow(img.astype('uint8'))\\nfig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\\nplt.show()\\n\""]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"G5lc3KUNpbCp","colab_type":"text"},"source":["# __1. Extract the images from the dataset stored on Drive__ \n","\n","We have already done so in our Operational.ipynb notebook\n","<br>\n","\n","## Some imports for the modules"]},{"cell_type":"code","metadata":{"id":"bNyuHgkTo0QB","colab_type":"code","colab":{}},"source":["import argparse\n","import json\n","import os\n","import shutil\n","import time\n","\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","\n","#from config import *\n","import data\n","from model import SCNN\n","from utils.tensorboard import TensorBoard\n","from utils.transforms import *\n","from utils.lr_scheduler import PolyLR"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tpwjo8h02zmq","colab_type":"text"},"source":["## Loading model configurations"]},{"cell_type":"code","metadata":{"id":"MKPduURx5Xn0","colab_type":"code","outputId":"d5bcf34b-0e9f-4edf-a334-b264b663a3ab","executionInfo":{"status":"ok","timestamp":1589889624591,"user_tz":420,"elapsed":1257,"user":{"displayName":"ASHAR ALAM","photoUrl":"","userId":"03610029456235520310"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["# Location of our configuration for training\n","exp_dir = EXPERIMENT_CONFIG + '/cfg.json'\n","exp_name = 'out_of_mem_test'\n","save_dir = MODEL_SAVE + '/' + exp_name\n","with open(exp_dir) as f:\n","  exp_cfg = json.load(f)\n","print(exp_cfg)\n","\n","# Extracting important configurations\n","resize_shape = tuple(exp_cfg['dataset']['resize_shape'])\n","\n","device = torch.device(exp_cfg['device'])\n","\n","######################## COMMAND CENTER LATER###############\n","#tensorboard = TensorBoard(exp_dir) # Command center Later\n","############################################################"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{'device': 'cuda:0', 'MAX_EPOCHES': 10, 'dataset': {'dataset_name': 'Tusimple', 'batch_size': 32, 'resize_shape': [512, 288]}, 'optim': {'lr': 0.15, 'momentum': 0.9, 'weight_decay': 0.0001, 'nesterov': True}, 'lr_scheduler': {'warmup': 20, 'max_iter': 1500, 'min_lrs': 1e-10}, 'model': {'scale_exist': 0.07}}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tg0NQPSfCapp","colab_type":"text"},"source":["# __2. Organize the images into raw images and labelled value__\n","\n","Here, we will generate labelled data for our train, validation and test splits. We will generate labelled data as bindary masks since the original dataset doesn't come preloaded with this information.\n","<br>\n","Moreover, instead of loading data manually, we decided on using Pytorch to create a custom Dataset and DataLoader\n","<br> \n","More details on the DataLoader setup can be found in `LaneNet/Data/Tusimple.py`\n"]},{"cell_type":"markdown","metadata":{"id":"ftpvZc1sDKxh","colab_type":"text"},"source":["### Data Pre-processing\n","We perform three data augmentation steps to our images:\n","<br>\n","1. Resizing images to make them smaller\n","2. Rotation by 2 degrees\n","3. Normalizing data by given mean and standard deviation\n"]},{"cell_type":"code","metadata":{"id":"xhpd5N7UCbcr","colab_type":"code","colab":{}},"source":["# Training data\n","# Predefined mean and std- deviation\n","mean=(0.485, 0.456, 0.406)\n","std=(0.229, 0.224, 0.225)\n","\n","transform_train = Compose(Resize(resize_shape), Rotation(2), ToTensor(),\n","                          Normalize(mean=mean, std=std))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QxH7OMYKDugW","colab_type":"text"},"source":["### Now we perform data loading operations on our custom defined Dataset Object `namely Tusimple`\n","Basically, this is where we create our __train, validation and test splits__ on our dataset"]},{"cell_type":"code","metadata":{"id":"-_QMJGFKD33n","colab_type":"code","outputId":"a9d2a70e-d110-4879-cd66-e7543623c21c","executionInfo":{"status":"ok","timestamp":1589889763650,"user_tz":420,"elapsed":7328,"user":{"displayName":"ASHAR ALAM","photoUrl":"","userId":"03610029456235520310"}},"colab":{"base_uri":"https://localhost:8080/","height":196}},"source":["dataset_name = exp_cfg['dataset']['dataset_name']\n","Dataset_Type = getattr(data, dataset_name)\n","\n","# Training Data\n","train_dataset = Dataset_Type(\"\", \"train\", transform_train)\n","train_loader = DataLoader(train_dataset, batch_size=exp_cfg['dataset']['batch_size'], shuffle=True, collate_fn=train_dataset.collate, num_workers=8)\n","\n","# Validation Data\n","transform_val_img = Resize(resize_shape)\n","transform_val_x = Compose(ToTensor(), Normalize(mean=mean, std=std))\n","transform_val = Compose(transform_val_img, transform_val_x)\n","val_dataset = Dataset_Type(\"\", \"val\", transform_val)\n","val_loader = DataLoader(val_dataset, batch_size=8, collate_fn=val_dataset.collate, num_workers=4)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Label is going to get generated into dir: seg_label ...\n","/content/drive/My Drive/cs231n/Project/dataset/trainset/seg_label\n","train set is done\n","val set is done\n","test set is done\n","Label is going to get generated into dir: seg_label ...\n","/content/drive/My Drive/cs231n/Project/dataset/trainset/seg_label\n","train set is done\n","val set is done\n","test set is done\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KA-uH7G_Iba4","colab_type":"text"},"source":["## Initializing our Neural Network Model\n"]},{"cell_type":"code","metadata":{"id":"z0tE79jWIfny","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85,"referenced_widgets":["7d1f17bd25354d64b8875765941392f8","2a636c0309d74e18a6ffa0aff6a570ac","7beff0e6b7c64c228702bd889aba5397","45d6aa555d434495a2a30c7f869645f8","f54e8a0c62574bdc8bccc5d52080a68e","0686325891204464ba65e5f984643c93","86151d47bd95482d803ad2a9b313fc67","ffc2a8d30278410b8e378d5e70029f07"]},"outputId":"84134c3c-d8b4-40f3-eeb1-9b3278c9bc3a","executionInfo":{"status":"ok","timestamp":1589889804139,"user_tz":420,"elapsed":46252,"user":{"displayName":"ASHAR ALAM","photoUrl":"","userId":"03610029456235520310"}}},"source":["net = SCNN(resize_shape, pretrained=True)\n","net = net.to(device)\n","net = torch.nn.DataParallel(net)\n","\n","optimizer = optim.SGD(net.parameters(), **exp_cfg['optim'])\n","lr_scheduler = PolyLR(optimizer, 0.9, **exp_cfg['lr_scheduler'])\n","best_val_loss = 1e6"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\" to /root/.cache/torch/checkpoints/vgg16_bn-6c64b313.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7d1f17bd25354d64b8875765941392f8","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=553507836.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"C0Dtg_wjCvtA","colab_type":"text"},"source":["### Understanding the Data Loader\n","\n","Let's see what's there in the DataLoader"]},{"cell_type":"code","metadata":{"id":"Np_lvR1mCu3B","colab_type":"code","outputId":"d978f180-9981-44f6-9686-537606584afa","executionInfo":{"status":"ok","timestamp":1589889353168,"user_tz":420,"elapsed":333,"user":{"displayName":"ASHAR ALAM","photoUrl":"","userId":"03610029456235520310"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["\"\"\"\n","from utils.prob2lines import getLane\n","\n","net.train()\n","train_loss = 0\n","train_loss_seg = 0\n","train_loss_exist = 0 \n","for batch_idx, sample in enumerate(val_loader):\n","  img = sample['img'].to(device)\n","  segLabel = sample['segLabel'].to(device)\n","  exist = sample['exist'].to(device)\n","  break\n","print(sample['segLabel'])\n","optimizer.zero_grad()\n","seg_pred, exist_pred = net(img)[:2]\n","seg_pred = F.softmax(seg_pred, dim=1)\n","seg_pred = seg_pred.detach().cpu().numpy()\n","exist_pred = exist_pred.detach().cpu().numpy()\n","\n","for b in range(len(seg_pred)):\n","  seg = seg_pred[b]\n","  exist = [1 if exist_pred[b, i] > 0.5 else 0 for i in range(4)]\n","  pred_lane_coords = getLane.prob2lines_tusimple(seg, exist, resize_shape=(720, 1280), y_px_gap=10, pts=56)\n","  for i in range(len(lane_coords)):\n","      lane_coords[i] = sorted(lane_coords[i], key=lambda pair: pair[1])\n","\n","\n","  for l in lane_coords:\n","      if len(l) == 0:\n","          continue\n","      json_dict['lanes'].append([])\n","      for (x, y) in l:\n","          json_dict['lanes'][-1].append(int(x))\n","  for (x, y) in lane_coords[0]:\n","      json_dict['h_sample'].append(y)\n","  dump_to_json.append(json.dumps(json_dict))\n","\"\"\""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nfrom utils.prob2lines import getLane\\n\\nnet.train()\\ntrain_loss = 0\\ntrain_loss_seg = 0\\ntrain_loss_exist = 0 \\nfor batch_idx, sample in enumerate(val_loader):\\n  img = sample['img'].to(device)\\n  segLabel = sample['segLabel'].to(device)\\n  exist = sample['exist'].to(device)\\n  break\\nprint(sample['segLabel'])\\noptimizer.zero_grad()\\nseg_pred, exist_pred = net(img)[:2]\\nseg_pred = F.softmax(seg_pred, dim=1)\\nseg_pred = seg_pred.detach().cpu().numpy()\\nexist_pred = exist_pred.detach().cpu().numpy()\\n\\nfor b in range(len(seg_pred)):\\n  seg = seg_pred[b]\\n  exist = [1 if exist_pred[b, i] > 0.5 else 0 for i in range(4)]\\n  pred_lane_coords = getLane.prob2lines_tusimple(seg, exist, resize_shape=(720, 1280), y_px_gap=10, pts=56)\\n  for i in range(len(lane_coords)):\\n      lane_coords[i] = sorted(lane_coords[i], key=lambda pair: pair[1])\\n\\n\\n  for l in lane_coords:\\n      if len(l) == 0:\\n          continue\\n      json_dict['lanes'].append([])\\n      for (x, y) in l:\\n          json_dict['lanes'][-1].append(int(x))\\n  for (x, y) in lane_coords[0]:\\n      json_dict['h_sample'].append(y)\\n  dump_to_json.append(json.dumps(json_dict))\\n\""]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"brf1U_yZIpo-","colab_type":"text"},"source":["## Defining our train function"]},{"cell_type":"code","metadata":{"id":"A50BpVJUIsYC","colab_type":"code","colab":{}},"source":["def train(epoch, train_loss_history):\n","    print(\"Train Epoch: {}\".format(epoch))\n","    net.train()\n","    train_loss = 0\n","    train_loss_seg = 0\n","    train_loss_exist = 0\n","    progressbar = tqdm(range(len(train_loader)))\n","\n","    for batch_idx, sample in enumerate(train_loader):\n","        img = sample['img'].to(device)\n","        segLabel = sample['segLabel'].to(device)\n","        exist = sample['exist'].to(device)\n","\n","        optimizer.zero_grad()\n","        seg_pred, exist_pred, loss_seg, loss_exist, loss = net(img, segLabel, exist)\n","        if isinstance(net, torch.nn.DataParallel):\n","            loss_seg = loss_seg.sum()\n","            loss_exist = loss_exist.sum()\n","            loss = loss.sum()\n","        loss.backward()\n","        optimizer.step()\n","        lr_scheduler.step()\n","\n","        iter_idx = epoch * len(train_loader) + batch_idx\n","        train_loss = loss.item()\n","        train_loss_seg = loss_seg.item()\n","        train_loss_exist = loss_exist.item()\n","        progressbar.set_description(\"batch loss: {:.3f}\".format(loss.item()))\n","        progressbar.update(1)\n","\n","        train_loss_history.append(train_loss)\n","        lr = optimizer.param_groups[0]['lr']\n","        \n","        # Command center later\n","        #tensorboard.scalar_summary(exp_name + \"/train_loss\", train_loss, iter_idx)\n","        #tensorboard.scalar_summary(exp_name + \"/train_loss_seg\", train_loss_seg, iter_idx)\n","        #tensorboard.scalar_summary(exp_name + \"/train_loss_exist\", train_loss_exist, iter_idx)\n","        #tensorboard.scalar_summary(exp_name + \"/learning_rate\", lr, iter_idx)\n","\n","    progressbar.close()\n","    #tensorboard.writer.flush()\n","\n","    if epoch % 1 == 0:\n","        save_dict = {\n","            \"epoch\": epoch,\n","            \"net\": net.module.state_dict() if isinstance(net, torch.nn.DataParallel) else net.state_dict(),\n","            \"optim\": optimizer.state_dict(),\n","            \"lr_scheduler\": lr_scheduler.state_dict(),\n","            \"best_val_loss\": best_val_loss\n","        }\n","        \n","        if not os.path.exists(save_dir):\n","          os.makedirs(save_dir)\n","        save_name = os.path.join(save_dir, exp_name + '.pth')\n","        torch.save(save_dict, save_name)\n","        print(\"model is saved: {}\".format(save_name))\n","\n","    print(\"------------------------\\n\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fBmUEVcRI8GW","colab_type":"text"},"source":["## Defining our validation function\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"NNKUTGPbI_xN","colab_type":"code","colab":{}},"source":["def val(epoch, validation_loss_history):\n","    global best_val_loss\n","\n","    print(\"Val Epoch: {}\".format(epoch))\n","\n","    net.eval()\n","    val_loss = 0\n","    val_loss_seg = 0\n","    val_loss_exist = 0\n","    progressbar = tqdm(range(len(val_loader)))\n","\n","    with torch.no_grad():\n","        for batch_idx, sample in enumerate(val_loader):\n","            img = sample['img'].to(device)\n","            segLabel = sample['segLabel'].to(device)\n","            exist = sample['exist'].to(device)\n","\n","            seg_pred, exist_pred, loss_seg, loss_exist, loss = net(img, segLabel, exist)\n","            \n","\n","            if isinstance(net, torch.nn.DataParallel):\n","                loss_seg = loss_seg.sum()\n","                loss_exist = loss_exist.sum()\n","                loss = loss.sum()\n","\n","            # visualize validation every 5 frame, 50 frames in all\n","            gap_num = 5\n","            if batch_idx%gap_num == 0 and batch_idx < 50 * gap_num:\n","                origin_imgs = []\n","                seg_pred = seg_pred.detach().cpu().numpy()\n","                exist_pred = exist_pred.detach().cpu().numpy()\n","\n","                for b in range(len(img)):\n","                    img_name = sample['img_name'][b]\n","                    img = cv2.imread(img_name)\n","                    img = transform_val_img({'img': img})['img']\n","\n","                    lane_img = np.zeros_like(img)\n","                    color = np.array([[255, 125, 0], [0, 255, 0], [0, 0, 255], [0, 255, 255]], dtype='uint8')\n","\n","                    coord_mask = np.argmax(seg_pred[b], axis=0)\n","                    for i in range(0, 4):\n","                        if exist_pred[b, i] > 0.5:\n","                            lane_img[coord_mask==(i+1)] = color[i]\n","                    img = cv2.addWeighted(src1=lane_img, alpha=0.8, src2=img, beta=1., gamma=0.)\n","                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","                    lane_img = cv2.cvtColor(lane_img, cv2.COLOR_BGR2RGB)\n","                    cv2.putText(lane_img, \"{}\".format([1 if exist_pred[b, i]>0.5 else 0 for i in range(4)]), (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 1.1, (255, 255, 255), 2)\n","                    origin_imgs.append(img)\n","                    origin_imgs.append(lane_img)\n","                #tensorboard.image_summary(\"img_{}\".format(batch_idx), origin_imgs, epoch)\n","            \n","            val_loss_history.append(loss.item())\n","            val_loss += loss.item()\n","            val_loss_seg += loss_seg.item()\n","            val_loss_exist += loss_exist.item()\n","\n","            progressbar.set_description(\"batch loss: {:.3f}\".format(loss.item()))\n","            progressbar.update(1)\n","\n","    progressbar.close()\n","    iter_idx = (epoch + 1) * len(train_loader)  # keep align with training process iter_idx\n","    #tensorboard.scalar_summary(\"val_loss\", val_loss, iter_idx)\n","    #tensorboard.scalar_summary(\"val_loss_seg\", val_loss_seg, iter_idx)\n","    #tensorboard.scalar_summary(\"val_loss_exist\", val_loss_exist, iter_idx)\n","    #tensorboard.writer.flush()\n","\n","    print(\"------------------------\\n\")\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        #if not os.path.exists(save_dir):\n","        #  os.makedirs(save_dir)\n","        save_name = os.path.join(save_dir, exp_name + '.pth')\n","        copy_name = os.path.join(save_dir, exp_name + '_best.pth')\n","        shutil.copyfile(save_name, copy_name)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YLr11epkJUKd","colab_type":"text"},"source":["# __3. Train our Model__\n","\n","Finally, we get to train our model on the images"]},{"cell_type":"code","metadata":{"id":"xk2mVlxPJPSH","colab_type":"code","outputId":"32b5819f-6cf1-494b-9503-7201c0afb93b","executionInfo":{"status":"ok","timestamp":1589890249067,"user_tz":420,"elapsed":431188,"user":{"displayName":"ASHAR ALAM","photoUrl":"","userId":"03610029456235520310"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Let's start training\n","start_epoch = 0\n","\n","train_loss_history = []\n","val_loss_history = []\n","#exp_cfg['MAX_EPOCHES'] = int(np.ceil(exp_cfg['lr_scheduler']['max_iter'] / len(train_loader)))\n","for epoch in range(start_epoch, exp_cfg['MAX_EPOCHES']):\n","    train(epoch, train_loss_history)\n","    if epoch % 1 == 0:\n","        print(\"\\nValidation For Experiment: \", exp_dir)\n","        print(time.strftime('%H:%M:%S', time.localtime()))\n","        val(epoch, val_loss_history)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/4 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train Epoch: 0\n"],"name":"stdout"},{"output_type":"stream","text":["batch loss: 0.940: 100%|██████████| 4/4 [01:50<00:00, 27.69s/it]\n","  0%|          | 0/2 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["model is saved: /content/drive/My Drive/cs231n/Project/out_of_mem_test/out_of_mem_test.pth\n","------------------------\n","\n","\n","Validation For Experiment:  /content/drive/My Drive/cs231n/Project/LaneNet/config/cfg.json\n","12:05:29\n","Val Epoch: 0\n"],"name":"stdout"},{"output_type":"stream","text":["batch loss: 1.466: 100%|██████████| 2/2 [00:21<00:00, 10.89s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["------------------------\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/4 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train Epoch: 1\n"],"name":"stdout"},{"output_type":"stream","text":["batch loss: 0.711: 100%|██████████| 4/4 [00:30<00:00,  7.64s/it]\n","  0%|          | 0/2 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["model is saved: /content/drive/My Drive/cs231n/Project/out_of_mem_test/out_of_mem_test.pth\n","------------------------\n","\n","\n","Validation For Experiment:  /content/drive/My Drive/cs231n/Project/LaneNet/config/cfg.json\n","12:06:23\n","Val Epoch: 1\n"],"name":"stdout"},{"output_type":"stream","text":["batch loss: 1.009: 100%|██████████| 2/2 [00:01<00:00,  1.97it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["------------------------\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/4 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train Epoch: 2\n"],"name":"stdout"},{"output_type":"stream","text":["batch loss: 0.567: 100%|██████████| 4/4 [00:30<00:00,  7.65s/it]\n","  0%|          | 0/2 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["model is saved: /content/drive/My Drive/cs231n/Project/out_of_mem_test/out_of_mem_test.pth\n","------------------------\n","\n","\n","Validation For Experiment:  /content/drive/My Drive/cs231n/Project/LaneNet/config/cfg.json\n","12:06:56\n","Val Epoch: 2\n"],"name":"stdout"},{"output_type":"stream","text":["batch loss: 0.704: 100%|██████████| 2/2 [00:01<00:00,  1.36it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["------------------------\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/4 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train Epoch: 3\n"],"name":"stdout"},{"output_type":"stream","text":["batch loss: 0.263: 100%|██████████| 4/4 [00:30<00:00,  7.66s/it]\n","  0%|          | 0/2 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["model is saved: /content/drive/My Drive/cs231n/Project/out_of_mem_test/out_of_mem_test.pth\n","------------------------\n","\n","\n","Validation For Experiment:  /content/drive/My Drive/cs231n/Project/LaneNet/config/cfg.json\n","12:07:29\n","Val Epoch: 3\n"],"name":"stdout"},{"output_type":"stream","text":["batch loss: 0.390: 100%|██████████| 2/2 [00:01<00:00,  1.89it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["------------------------\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/4 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train Epoch: 4\n"],"name":"stdout"},{"output_type":"stream","text":["batch loss: 0.292: 100%|██████████| 4/4 [00:30<00:00,  7.56s/it]\n","  0%|          | 0/2 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["model is saved: /content/drive/My Drive/cs231n/Project/out_of_mem_test/out_of_mem_test.pth\n","------------------------\n","\n","\n","Validation For Experiment:  /content/drive/My Drive/cs231n/Project/LaneNet/config/cfg.json\n","12:08:02\n","Val Epoch: 4\n"],"name":"stdout"},{"output_type":"stream","text":["batch loss: 0.693: 100%|██████████| 2/2 [00:01<00:00,  1.82it/s]\n","  0%|          | 0/4 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["------------------------\n","\n","Train Epoch: 5\n"],"name":"stdout"},{"output_type":"stream","text":["batch loss: 0.269: 100%|██████████| 4/4 [00:30<00:00,  7.69s/it]\n","  0%|          | 0/2 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["model is saved: /content/drive/My Drive/cs231n/Project/out_of_mem_test/out_of_mem_test.pth\n","------------------------\n","\n","\n","Validation For Experiment:  /content/drive/My Drive/cs231n/Project/LaneNet/config/cfg.json\n","12:08:35\n","Val Epoch: 5\n"],"name":"stdout"},{"output_type":"stream","text":["batch loss: 0.622: 100%|██████████| 2/2 [00:01<00:00,  1.85it/s]\n","  0%|          | 0/4 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["------------------------\n","\n","Train Epoch: 6\n"],"name":"stdout"},{"output_type":"stream","text":["batch loss: 0.160: 100%|██████████| 4/4 [00:30<00:00,  7.68s/it]\n","  0%|          | 0/2 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["model is saved: /content/drive/My Drive/cs231n/Project/out_of_mem_test/out_of_mem_test.pth\n","------------------------\n","\n","\n","Validation For Experiment:  /content/drive/My Drive/cs231n/Project/LaneNet/config/cfg.json\n","12:09:08\n","Val Epoch: 6\n"],"name":"stdout"},{"output_type":"stream","text":["batch loss: 0.456: 100%|██████████| 2/2 [00:01<00:00,  1.84it/s]\n","  0%|          | 0/4 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["------------------------\n","\n","Train Epoch: 7\n"],"name":"stdout"},{"output_type":"stream","text":["batch loss: 0.151: 100%|██████████| 4/4 [00:30<00:00,  7.69s/it]\n","  0%|          | 0/2 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["model is saved: /content/drive/My Drive/cs231n/Project/out_of_mem_test/out_of_mem_test.pth\n","------------------------\n","\n","\n","Validation For Experiment:  /content/drive/My Drive/cs231n/Project/LaneNet/config/cfg.json\n","12:09:40\n","Val Epoch: 7\n"],"name":"stdout"},{"output_type":"stream","text":["batch loss: 0.217: 100%|██████████| 2/2 [00:01<00:00,  1.89it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["------------------------\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/4 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train Epoch: 8\n"],"name":"stdout"},{"output_type":"stream","text":["batch loss: 0.122: 100%|██████████| 4/4 [00:30<00:00,  7.68s/it]\n","  0%|          | 0/2 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["model is saved: /content/drive/My Drive/cs231n/Project/out_of_mem_test/out_of_mem_test.pth\n","------------------------\n","\n","\n","Validation For Experiment:  /content/drive/My Drive/cs231n/Project/LaneNet/config/cfg.json\n","12:10:13\n","Val Epoch: 8\n"],"name":"stdout"},{"output_type":"stream","text":["batch loss: 0.161: 100%|██████████| 2/2 [00:01<00:00,  1.84it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["------------------------\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/4 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train Epoch: 9\n"],"name":"stdout"},{"output_type":"stream","text":["batch loss: 0.151: 100%|██████████| 4/4 [00:30<00:00,  7.62s/it]\n","  0%|          | 0/2 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["model is saved: /content/drive/My Drive/cs231n/Project/out_of_mem_test/out_of_mem_test.pth\n","------------------------\n","\n","\n","Validation For Experiment:  /content/drive/My Drive/cs231n/Project/LaneNet/config/cfg.json\n","12:10:46\n","Val Epoch: 9\n"],"name":"stdout"},{"output_type":"stream","text":["batch loss: 0.130: 100%|██████████| 2/2 [00:01<00:00,  1.87it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["------------------------\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xWfVFwxtw2og","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dYVP43mnPtEX","colab_type":"code","outputId":"c2784229-443b-4739-fb61-ae02a3f3a205","executionInfo":{"status":"ok","timestamp":1589890336847,"user_tz":420,"elapsed":645,"user":{"displayName":"ASHAR ALAM","photoUrl":"","userId":"03610029456235520310"}},"colab":{"base_uri":"https://localhost:8080/","height":295}},"source":["# Visualizing the loss\n","#print(train_loss_history)\n","#print(val_loss_history)\n","\n","plt.title('Visualizing Loss')\n","plt.plot(train_loss_history, 'o', label=\"training_loss\")\n","plt.plot(val_loss_history, 'o', label=\"validation_loss\")\n","plt.xlabel('Iteration')\n","plt.legend(loc='upper center', ncol=4)\n","plt.show()"],"execution_count":13,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"BiSVddCdb00s","colab_type":"text"},"source":["# __5. Test our Model on unseen images__\n","\n","Let's finally test our model"]},{"cell_type":"code","metadata":{"id":"hzyOI54cb0HA","colab_type":"code","outputId":"7d3a5f69-d3eb-4b00-e950-9439af074a38","executionInfo":{"status":"ok","timestamp":1589881209631,"user_tz":420,"elapsed":17836,"user":{"displayName":"ASHAR ALAM","photoUrl":"","userId":"03610029456235520310"}},"colab":{"base_uri":"https://localhost:8080/","height":269}},"source":["import argparse\n","import json\n","import os\n","\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","\n","import data\n","from model import SCNN\n","from utils.prob2lines import getLane\n","from utils.transforms import *\n","\n","# Location of our configuration for training\n","exp_dir = EXPERIMENT_CONFIG + '/cfg.json'\n","exp_name = '0'\n","save_dir = MODEL_SAVE + '/' + exp_name\n","with open(exp_dir) as f:\n","  exp_cfg = json.load(f)\n","print(exp_cfg)\n","\n","def split_path(path):\n","    \"\"\"split path tree into list\"\"\"\n","    folders = []\n","    while True:\n","        path, folder = os.path.split(path)\n","        if folder != \"\":\n","            folders.insert(0, folder)\n","        else:\n","            if path != \"\":\n","                folders.insert(0, path)\n","            break\n","    return folders\n","\n","# Extracting important configurations\n","resize_shape = tuple(exp_cfg['dataset']['resize_shape'])\n","\n","device = torch.device(exp_cfg['device'])\n","\n","mean = (0.485, 0.456, 0.406)\n","std = (0.229, 0.224, 0.225)\n","transform = Compose(Resize(resize_shape), ToTensor(),\n","                    Normalize(mean=mean, std=std))\n","\n","\n","dataset_name = exp_cfg['dataset']['dataset_name']\n","Dataset_Type = getattr(data, dataset_name)\n","\n","# Test Data\n","test_dataset = Dataset_Type(\"\", \"test\", transform)\n","test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=test_dataset.collate, num_workers=4)\n","\n","net = SCNN(input_size=resize_shape, pretrained=False)\n","save_name = os.path.join(save_dir, exp_name + '_best.pth')\n","save_dict = torch.load(save_name, map_location='cpu')\n","print(\"\\nloading\", save_name, \"...... From Epoch: \", save_dict['epoch'])\n","net.load_state_dict(save_dict['net'])\n","net = torch.nn.DataParallel(net.to(device))\n","net.eval()\n","\n","# ------------ test ------------\n","out_path = os.path.join(save_dir, \"coord_output\")\n","evaluation_path = os.path.join(save_dir, \"evaluate\")\n","if not os.path.exists(out_path):\n","    os.mkdir(out_path)\n","if not os.path.exists(evaluation_path):\n","    os.mkdir(evaluation_path)\n","dump_to_json = []\n","\n","progressbar = tqdm(range(len(test_loader)))\n","with torch.no_grad():\n","    for batch_idx, sample in enumerate(test_loader):\n","        img = sample['img'].to(device)\n","        img_name = sample['img_name']\n","\n","        seg_pred, exist_pred = net(img)[:2]\n","        seg_pred = F.softmax(seg_pred, dim=1)\n","        seg_pred = seg_pred.detach().cpu().numpy()\n","        exist_pred = exist_pred.detach().cpu().numpy()\n","\n","        for b in range(len(seg_pred)):\n","            seg = seg_pred[b]\n","            exist = [1 if exist_pred[b, i] > 0.5 else 0 for i in range(4)]\n","            lane_coords = getLane.prob2lines_tusimple(seg, exist, resize_shape=(720, 1280), y_px_gap=10, pts=56)\n","            for i in range(len(lane_coords)):\n","                lane_coords[i] = sorted(lane_coords[i], key=lambda pair: pair[1])\n","\n","            path_tree = split_path(img_name[b])\n","            save_dir, save_name = path_tree[-3:-1], path_tree[-1]\n","            save_dir = os.path.join(out_path, *save_dir)\n","            save_name = save_name[:-3] + \"lines.txt\"\n","            save_name = os.path.join(save_dir, save_name)\n","            if not os.path.exists(save_dir):\n","                os.makedirs(save_dir, exist_ok=True)\n","\n","            with open(save_name, \"w\") as f:\n","                for l in lane_coords:\n","                    for (x, y) in l:\n","                        print(\"{} {}\".format(x, y), end=\" \", file=f)\n","                    print(file=f)\n","\n","            json_dict = {}\n","            json_dict['lanes'] = []\n","            json_dict['h_sample'] = []\n","            json_dict['raw_file'] = os.path.join(*path_tree[-4:])\n","            json_dict['run_time'] = 0\n","            for l in lane_coords:\n","                if len(l) == 0:\n","                    continue\n","                json_dict['lanes'].append([])\n","                for (x, y) in l:\n","                    json_dict['lanes'][-1].append(int(x))\n","            for (x, y) in lane_coords[0]:\n","                json_dict['h_sample'].append(y)\n","            dump_to_json.append(json.dumps(json_dict))\n","\n","        progressbar.update(1)\n","progressbar.close()\n","\n","with open(os.path.join(out_path, \"predict_test.json\"), \"w\") as f:\n","    for line in dump_to_json:\n","        print(line, end=\"\\n\", file=f)\n","\n","# ---- evaluate ----\n","from utils.lane_evaluation.tusimple.lane import LaneEval\n","\n","print(save_dir)\n","\n","eval_result = LaneEval.bench_one_submit(os.path.join(out_path, \"predict_test.json\"),\n","                                        DATASET_PATH + \"/label_0601.json\")\n","print(eval_result)\n","with open(os.path.join(evaluation_path, \"evaluation_result.txt\"), \"w\") as f:\n","    print(eval_result, file=f)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{'device': 'cuda:0', 'MAX_EPOCHES': 10, 'dataset': {'dataset_name': 'Tusimple', 'batch_size': 32, 'resize_shape': [512, 288]}, 'optim': {'lr': 0.15, 'momentum': 0.9, 'weight_decay': 0.0001, 'nesterov': True}, 'lr_scheduler': {'warmup': 20, 'max_iter': 1500, 'min_lrs': 1e-10}, 'model': {'scale_exist': 0.07}}\n","Label is going to get generated into dir: seg_label ...\n","/content/drive/My Drive/cs231n/Project/dataset/trainset/seg_label\n","train set is done\n","val set is done\n","test set is done\n"],"name":"stdout"},{"output_type":"stream","text":["\n","  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n","loading /content/drive/My Drive/cs231n/Project/0/0_best.pth ...... From Epoch:  3\n"],"name":"stdout"},{"output_type":"stream","text":["\n","100%|██████████| 1/1 [00:11<00:00, 11.12s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["/content/drive/My Drive/cs231n/Project/0/coord_output/0601/1494453521596643824\n","[{\"name\": \"Accuracy\", \"value\": 0.7589285714285715, \"order\": \"desc\"}, {\"name\": \"FP\", \"value\": 0.85, \"order\": \"asc\"}, {\"name\": \"FN\", \"value\": 0.85, \"order\": \"asc\"}]\n"],"name":"stdout"}]}]}