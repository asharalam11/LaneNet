{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"custom_dataset.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM9Ex0RNw5CuGLSuATYu7C0"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"nokVIUZY5npU","colab_type":"text"},"source":["# Here we test Custom dataset generation\n","First importing as always"]},{"cell_type":"code","metadata":{"id":"RIDQH2K15exL","colab_type":"code","outputId":"114104a1-92bc-44ab-865a-6d544aee200d","executionInfo":{"status":"ok","timestamp":1590216890101,"user_tz":420,"elapsed":1175,"user":{"displayName":"ASHAR ALAM","photoUrl":"","userId":"03610029456235520310"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","from os.path import join\n","import os\n","\n","ROOT = '/content/drive'     # default for the drive\n","CODE = 'My Drive/cs231n/Project/LaneNet'       # path to our code on Drive\n","\n","# this mounts your Google Drive to the Colab VM.\n","drive.mount(ROOT)           # we mount the drive at /content/drive\n","\n","CODE_PATH = join(ROOT, CODE)\n","\n","# now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('{}'.format(CODE_PATH))\n","\n","#os.chdir(CODE_PATH)                        # Change directory to the location defined in code_path (Note cd doesn't always work)\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FTvYOTSX69sm","colab_type":"text"},"source":["# Now we will try creating our dataset\n","\n","First we will test whether we can load our datasets from within .py files (Till now, we have only been successful with .ipynb files).\n","<br>\n","We will first import our dataset by making a custom Dataset in Pytorch using the file `LaneNet/data/Tusimple.py`"]},{"cell_type":"code","metadata":{"id":"p346QcMp68ZV","colab_type":"code","outputId":"2a8b47a0-9280-44b1-b6b7-a71a85ce66d0","executionInfo":{"status":"ok","timestamp":1589764441758,"user_tz":420,"elapsed":1459,"user":{"displayName":"ASHAR ALAM","photoUrl":"","userId":"03610029456235520310"}},"colab":{"base_uri":"https://localhost:8080/","height":106}},"source":["#print(sys.path)\n","TEST_PATH = '/content/drive/My Drive/cs231n/Project/test'\n","\n","from data import Tusimple\n","\n","test = Tusimple.Tusimple(TEST_PATH,'train')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Label is going to get generated into dir: /content/drive/My Drive/cs231n/Project/test/seg_label ...\n","/content/drive/My Drive/cs231n/Project/dataset/trainset/clips/seg_label\n","train set is done\n","val set is done\n","test set is done\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iTCFtazCT1-2","colab_type":"text"},"source":["# __We will try to create custom train, val, test splits with 100, 10 and 10 images respectively__\n"]},{"cell_type":"code","metadata":{"id":"BxQ0R6iLT-4v","colab_type":"code","outputId":"ce4bc2e4-837f-43e3-bba3-afddd12516f0","executionInfo":{"status":"ok","timestamp":1590221963853,"user_tz":420,"elapsed":2832,"user":{"displayName":"ASHAR ALAM","photoUrl":"","userId":"03610029456235520310"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["import json\n","import os\n","\n","import cv2\n","import numpy as np\n","import torch\n","\n","train = 'label_0313_train.json'\n","val = 'label_0531_val.json'\n","test = 'label_0601_test.json'\n","\n","train_accuracy = 'train_accuracy.json'\n","val_accuracy = 'val_accuracy.json'\n","test_accuracy = 'test_accuracy.json'\n","\n","# This is where the main files are kept\n","train_set = 'label_data_0313.json'\n","val_set = 'label_data_0531.json'\n","test_set = 'label_data_0601.json'\n","data_dir_path = '/content/drive/My Drive/cs231n/Project/dataset/trainset/clips'\n","out_path = '/content/drive/My Drive/cs231n/Project/dataset/trainset'\n","\n","def write_json_accuracy(filename_read, filename_write, num_entries):\n","  PATH_TO_READ = os.path.join(data_dir_path, filename_read)\n","  ctr = 0                       # Initializing counter to 0\n","  dump_to_json = []             # We will dump these entries to a new json file\n","  with open(PATH_TO_READ) as f:\n","    for line in f:    \n","      if ctr >= num_entries:     # Once we have read enough entries\n","        break\n","      json_dict = json.loads(line)\n","      # Making the lane dictionary\n","      lane_dictionary = {}\n","      lane_dictionary[ctr] = json_dict[\"lanes\"]\n","      lane_dictionary[-1] = json_dict[\"h_samples\"]\n","      dump_to_json.append(lane_dictionary)\n","      #dump_to_json.append(json.dumps(make_dictionary))\n","      ctr += 1                  # Increment the counter\n","  # Now we will write to the given json file\n","  # Dumping a json dictionary at once\n","  with open(os.path.join(out_path, filename_write), \"w\") as f:\n","    #for line in dump_to_json:\n","        #print(line, end=\"\\n\", file=f)\n","    json.dump(dump_to_json, f)\n","\n","def write_json(filename_read, filename_write, num_entries):\n","  PATH_TO_READ = os.path.join(data_dir_path, filename_read)\n","  ctr = 0                       # Initializing counter to 0\n","  dump_to_json = []             # We will dump these entries to a new json file\n","  with open(PATH_TO_READ) as f:\n","    for line in f:    \n","      if ctr >= num_entries:     # Once we have read enough entries\n","        break\n","      json_dict = json.loads(line)\n","      dump_to_json.append(json.dumps(json_dict))\n","      ctr += 1                  # Increment the counter\n","  # Now we will write to the given json file\n","  with open(os.path.join(out_path, filename_write), \"w\") as f:\n","    for line in dump_to_json:\n","        print(line, end=\"\\n\", file=f)\n","    \n","\n","num_train = 1000\n","num_val = 100\n","num_test = 100\n","\n","write_json(train_set, train_accuracy, num_train)   # Make custom training data\n","write_json(val_set, val, num_val)   # Make custom validation data\n","write_json(test_set, test, num_test)   # Make custom test data\n","\n","# Writing the accuracy files for help in gaining training accuracy\n","write_json_accuracy(train_set, train_accuracy, num_train)\n","write_json_accuracy(val_set, val_accuracy, num_val)\n","write_json_accuracy(test_set, test_accuracy, num_val)\n","\n","\"\"\"\n","PATH_TO_READ = os.path.join(out_path, train_accuracy)\n","with open(PATH_TO_READ) as f:\n","  ctr = str(5)\n","  h_sample = str(-1)\n","  json_dict = json.load(f)\n","  #print(json_dict)\n","  print(json_dict[5][h_sample])\n","\"\"\""],"execution_count":65,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nPATH_TO_READ = os.path.join(out_path, train_accuracy)\\nwith open(PATH_TO_READ) as f:\\n  ctr = str(5)\\n  h_sample = str(-1)\\n  json_dict = json.load(f)\\n  #print(json_dict)\\n  print(json_dict[5][h_sample])\\n'"]},"metadata":{"tags":[]},"execution_count":65}]}]}