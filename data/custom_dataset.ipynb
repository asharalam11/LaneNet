{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nokVIUZY5npU"
   },
   "source": [
    "# Here we test Custom dataset generation\n",
    "First importing as always"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'posixpath' from '/opt/anaconda3/lib/python3.7/posixpath.py'>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1175,
     "status": "ok",
     "timestamp": 1590216890101,
     "user": {
      "displayName": "ASHAR ALAM",
      "photoUrl": "",
      "userId": "03610029456235520310"
     },
     "user_tz": 420
    },
    "id": "RIDQH2K15exL",
    "outputId": "114104a1-92bc-44ab-865a-6d544aee200d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/ashar/LaneNet/data', '/opt/anaconda3/lib/python37.zip', '/opt/anaconda3/lib/python3.7', '/opt/anaconda3/lib/python3.7/lib-dynload', '', '/opt/anaconda3/lib/python3.7/site-packages', '/opt/anaconda3/lib/python3.7/site-packages/IPython/extensions', '/home/ashar/.ipython', '/home/ashar/LaneNet']\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "from os.path import join\n",
    "import os\n",
    "\n",
    "ROOT = '/home/ashar'     # default for the drive\n",
    "CODE = 'LaneNet'       # path to our code on Drive\n",
    "\n",
    "CODE_PATH = join(ROOT, CODE)\n",
    "\n",
    "# now that we've mounted your Drive, this ensures that\n",
    "# the Python interpreter of the Colab VM can load\n",
    "# python files from within it.\n",
    "import sys\n",
    "sys.path.append('{}'.format(CODE_PATH))\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FTvYOTSX69sm"
   },
   "source": [
    "# Now we will try creating our dataset\n",
    "\n",
    "First we will test whether we can load our datasets from within .py files (Till now, we have only been successful with .ipynb files).\n",
    "<br>\n",
    "We will first import our dataset by making a custom Dataset in Pytorch using the file `LaneNet/data/Tusimple.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1459,
     "status": "ok",
     "timestamp": 1589764441758,
     "user": {
      "displayName": "ASHAR ALAM",
      "photoUrl": "",
      "userId": "03610029456235520310"
     },
     "user_tz": 420
    },
    "id": "p346QcMp68ZV",
    "outputId": "2a8b47a0-9280-44b1-b6b7-a71a85ce66d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label is going to get generated into dir: /content/drive/My Drive/cs231n/Project/test/seg_label ...\n",
      "/content/drive/My Drive/cs231n/Project/dataset/trainset/clips/seg_label\n",
      "train set is done\n",
      "val set is done\n",
      "test set is done\n"
     ]
    }
   ],
   "source": [
    "#print(sys.path)\n",
    "DATASET_PATH = '/home/ashar/dataset/trainset'\n",
    "\n",
    "from data import Tusimple\n",
    "\n",
    "test = Tusimple.Tusimple(TEST_PATH,'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iTCFtazCT1-2"
   },
   "source": [
    "# __We will try to create custom train, val, test splits with 100, 10 and 10 images respectively__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2832,
     "status": "ok",
     "timestamp": 1590221963853,
     "user": {
      "displayName": "ASHAR ALAM",
      "photoUrl": "",
      "userId": "03610029456235520310"
     },
     "user_tz": 420
    },
    "id": "BxQ0R6iLT-4v",
    "outputId": "ce4bc2e4-837f-43e3-bba3-afddd12516f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnum_train = 1000\\nnum_val = 100\\nnum_test = 100\\n\\nwrite_json(train_set, train, num_train)   # Make custom training data\\nwrite_json(val_set, val, num_val)   # Make custom validation data\\nwrite_json(test_set, test, num_test)   # Make custom test data\\n\\n# Writing the accuracy files for help in gaining training accuracy\\nwrite_json_accuracy(train_set, train_accuracy, num_train)\\nwrite_json_accuracy(val_set, val_accuracy, num_val)\\nwrite_json_accuracy(test_set, test_accuracy, num_val)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "train = 'label_train_16.json'\n",
    "val = 'label_val_16.json'\n",
    "test = 'label_test_16.json'\n",
    "\n",
    "train_accuracy = 'train_accuracy_16.json'\n",
    "val_accuracy = 'val_accuracy_16.json'\n",
    "test_accuracy = 'test_accuracy_16.json'\n",
    "\n",
    "# This is where the main files are kept\n",
    "train_set = 'label_data_0313.json'\n",
    "val_set = 'label_data_0531.json'\n",
    "test_set = 'label_data_0601.json'\n",
    "\n",
    "data_dir_path = '/home/ashar/dataset/trainset'\n",
    "out_path = '/home/ashar/dataset/trainset'\n",
    "\n",
    "def write_json_accuracy(filename_read, filename_write):\n",
    "  PATH_TO_READ = os.path.join(data_dir_path, filename_read)\n",
    "  ctr = 0                       # Initializing counter to 0\n",
    "  dump_to_json = []             # We will dump these entries to a new json file\n",
    "  with open(PATH_TO_READ) as f:\n",
    "    for line in f:    \n",
    "      json_dict = json.loads(line)\n",
    "      # Making the lane dictionary\n",
    "      lane_dictionary = {}\n",
    "      lane_dictionary[ctr] = json_dict[\"lanes\"]\n",
    "      lane_dictionary[-1] = json_dict[\"h_samples\"]\n",
    "      dump_to_json.append(lane_dictionary)\n",
    "  # Now we will write to the given json file\n",
    "  # Dumping a json dictionary at once\n",
    "  with open(os.path.join(out_path, filename_write), \"w\") as f:\n",
    "    #for line in dump_to_json:\n",
    "        #print(line, end=\"\\n\", file=f)\n",
    "    json.dump(dump_to_json, f)\n",
    "\n",
    "def write_json(filename_read, filename_write, num_entries):\n",
    "  PATH_TO_READ = os.path.join(data_dir_path, filename_read)\n",
    "  ctr = 0                       # Initializing counter to 0\n",
    "  dump_to_json = []             # We will dump these entries to a new json file\n",
    "  with open(PATH_TO_READ) as f:\n",
    "    for line in f:    \n",
    "      if ctr >= num_entries:     # Once we have read enough entries\n",
    "        break\n",
    "      json_dict = json.loads(line)\n",
    "      dump_to_json.append(json.dumps(json_dict))\n",
    "      ctr += 1                  # Increment the counter\n",
    "  # Now we will write to the given json file\n",
    "  with open(os.path.join(out_path, filename_write), \"w\") as f:\n",
    "    for line in dump_to_json:\n",
    "        print(line, end=\"\\n\", file=f)\n",
    "\n",
    "def write_json_accuracy_16(filename_read, filename_write, num_entries):\n",
    "  PATH_TO_READ = os.path.join(data_dir_path, filename_read)\n",
    "  ctr = 0                       # Initializing counter to 0\n",
    "  dump_to_json = []             # We will dump these entries to a new json file\n",
    "  with open(PATH_TO_READ) as f:\n",
    "    for line in f:    \n",
    "      if ctr >= num_entries:     # Once we have read enough entries\n",
    "        break\n",
    "      json_dict = json.loads(line)\n",
    "      # Making the lane dictionary\n",
    "      lane_dictionary = {}\n",
    "      lane_dictionary[ctr] = json_dict[\"lanes\"]\n",
    "      lane_dictionary[-1] = json_dict[\"h_samples\"]\n",
    "      dump_to_json.append(lane_dictionary)\n",
    "      #dump_to_json.append(json.dumps(make_dictionary))\n",
    "      ctr += 1                  # Increment the counter\n",
    "  # Now we will write to the given json file\n",
    "  # Dumping a json dictionary at once\n",
    "  with open(os.path.join(out_path, filename_write), \"w\") as f:\n",
    "    #for line in dump_to_json:\n",
    "        #print(line, end=\"\\n\", file=f)\n",
    "    json.dump(dump_to_json, f)\n",
    "\n",
    "    \n",
    "def write_json_16(filename1, filename2, filename_write, num_entries):\n",
    "  PATH_TO_READ_1 = os.path.join(data_dir_path, filename1)\n",
    "  PATH_TO_READ_2 = os.path.join(data_dir_path, filename2)\n",
    "  dump_to_json = []             # We will dump these entries to a new json file\n",
    "  ctr = 0\n",
    "  with open(PATH_TO_READ_1) as f:\n",
    "    for line in f:    \n",
    "      json_dict = json.loads(line)\n",
    "      dump_to_json.append(json.dumps(json_dict))\n",
    "  # Now doing the same with the second file\n",
    "  with open(PATH_TO_READ_2) as f:\n",
    "    for line in f:\n",
    "      if ctr >= num_entries:\n",
    "        break\n",
    "      json_dict = json.loads(line)\n",
    "      dump_to_json.append(json.dumps(json_dict))\n",
    "      ctr += 1\n",
    "  # Now we will write to the given json file\n",
    "  with open(os.path.join(out_path, filename_write), \"w\") as f:\n",
    "    for line in dump_to_json:\n",
    "        print(line, end=\"\\n\", file=f)\n",
    "\n",
    "num_train = 342\n",
    "num_val = 384\n",
    "write_json_16(train_set, val_set, train, num_train)   # Make custom training data\n",
    "write_json(test_set, val, num_val)   # Make custom validation data\n",
    "\n",
    "# Writing the accuracy files for help in gaining training accuracy\n",
    "write_json_accuracy(train, train_accuracy)\n",
    "write_json_accuracy(val, val_accuracy)\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "num_train = 1000\n",
    "num_val = 100\n",
    "num_test = 100\n",
    "\n",
    "write_json(train_set, train, num_train)   # Make custom training data\n",
    "write_json(val_set, val, num_val)   # Make custom validation data\n",
    "write_json(test_set, test, num_test)   # Make custom test data\n",
    "\n",
    "# Writing the accuracy files for help in gaining training accuracy\n",
    "write_json_accuracy(train_set, train_accuracy, num_train)\n",
    "write_json_accuracy(val_set, val_accuracy, num_val)\n",
    "write_json_accuracy(test_set, test_accuracy, num_val)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM9Ex0RNw5CuGLSuATYu7C0",
   "collapsed_sections": [],
   "name": "custom_dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
